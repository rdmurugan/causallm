"""
Integration tests to verify CausalLLM meets documentation requirements
Tests end-to-end functionality as described in README and documentation
"""
import pytest
import numpy as np
import pandas as pd
from unittest.mock import Mock, patch, AsyncMock
from causallm import CausalLLM
from causallm.core.statistical_methods import PCAlgorithm, ConditionalIndependenceTest, bootstrap_stability_test
from causallm.core.causal_discovery import discover_causal_structure, DiscoveryMethod
from causallm.plugins.slm_support import create_slm_optimized_client


class TestDocumentationExamples:
    """Test examples from README and documentation."""
    
    @patch('causallm.core.create_llm_client')
    def test_quick_start_example(self, mock_create_client):
        """Test the quick start example from README."""
        mock_client = Mock()
        mock_create_client.return_value = mock_client
        
        # Initialize CausalLLM as shown in README
        causallm = CausalLLM()
        
        assert causallm is not None
        assert causallm.llm_client is mock_client
        assert causallm.discovery_engine is not None
        assert causallm.dag_parser is not None
        assert causallm.do_operator is not None
        assert causallm.counterfactual_engine is not None
    
    @pytest.mark.asyncio
    @patch('causallm.core.create_llm_client')
    async def test_discover_causal_relationships_example(self, mock_create_client, sample_data):
        """Test causal relationship discovery as shown in README."""
        mock_client = Mock()
        mock_create_client.return_value = mock_client
        
        # Mock discovery result
        mock_result = Mock()
        mock_result.discovered_edges = []
        mock_result.confidence_summary = {}
        mock_result.causal_edges = []  # For compatibility
        
        causallm = CausalLLM()
        causallm.discovery_engine.discover_relationships = AsyncMock(return_value=mock_result)
        
        # Discover causal relationships as shown in README
        result = await causallm.discover_causal_relationships(
            data=sample_data,
            variables=["treatment", "outcome", "age", "income"]
        )
        
        assert result is mock_result
        
        # Test the expected interface
        causal_edges = getattr(result, 'causal_edges', result.discovered_edges)
        print(f"Found {len(causal_edges)} causal relationships")
        
        assert isinstance(causal_edges, list)
    
    @pytest.mark.asyncio 
    @patch('causallm.core.create_llm_client')
    async def test_basic_causal_analysis_example(self, mock_create_client, sample_data):
        """Test basic causal analysis example from README."""
        mock_client = Mock()
        mock_create_client.return_value = mock_client
        
        # Initialize with OpenAI provider as shown in README
        causallm = CausalLLM(llm_client=mock_client)
        
        # Mock discovery result
        mock_structure = Mock()
        mock_structure.discovered_edges = []
        causallm.discovery_engine.discover_relationships = AsyncMock(return_value=mock_structure)
        
        # Mock effect estimation
        mock_effect = Mock()
        mock_effect.estimate = 0.523
        mock_effect.std_error = 0.087
        causallm.do_operator.estimate_effect = AsyncMock(return_value=mock_effect)
        
        # Discover causal structure
        structure = await causallm.discover_causal_relationships(
            data=sample_data,
            target_variable="sales",
            domain="business"
        )
        
        # Estimate causal effect
        effect = await causallm.estimate_causal_effect(
            data=sample_data,
            treatment="marketing_spend",
            outcome="sales",
            confounders=["seasonality", "competition"]
        )
        
        # Verify expected format
        print(f"Causal effect: {effect.estimate:.3f} Â± {effect.std_error:.3f}")
        
        assert hasattr(effect, 'estimate')
        assert hasattr(effect, 'std_error')
        assert isinstance(effect.estimate, (int, float))
        assert isinstance(effect.std_error, (int, float))
    
    def test_statistical_methods_example(self, sample_data):
        """Test statistical methods example from README."""
        # Use pure statistical approach as shown in README
        ci_test = ConditionalIndependenceTest(method="partial_correlation")
        pc = PCAlgorithm(ci_test=ci_test, max_conditioning_size=3)
        
        # Discover causal skeleton
        skeleton = pc.discover_skeleton(sample_data)
        dag = pc.orient_edges(skeleton, sample_data)
        
        assert isinstance(skeleton, object)  # NetworkX Graph
        assert isinstance(dag, object)  # NetworkX DiGraph
        
        # Test stability with bootstrap
        stable_graph, stability_scores = bootstrap_stability_test(
            sample_data, pc, n_bootstrap=10  # Reduced for speed
        )
        
        assert isinstance(stable_graph, object)
        assert isinstance(stability_scores, dict)
    
    @patch('causallm.plugins.slm_support.create_slm_optimized_client')
    @patch('causallm.core.create_llm_client')
    def test_slm_support_example(self, mock_create_client, mock_create_slm):
        """Test Small Language Models example from README."""
        mock_slm_client = Mock()
        mock_create_slm.return_value = mock_slm_client
        
        # Create SLM-optimized client as shown in README
        slm_client = create_slm_optimized_client("llama2-7b")
        causallm = CausalLLM(llm_client=slm_client)
        
        assert causallm.llm_client is mock_slm_client
        mock_create_slm.assert_called_once_with("llama2-7b")
    
    @pytest.mark.asyncio
    async def test_discover_causal_structure_convenience_function(self, variable_descriptions):
        """Test the convenience function for causal structure discovery."""
        mock_result = Mock()
        mock_result.discovered_edges = []
        
        with patch('causallm.core.causal_discovery.AdvancedCausalDiscovery') as mock_discovery_class:
            mock_discovery = Mock()
            mock_discovery.discover = AsyncMock(return_value=mock_result)
            mock_discovery_class.return_value = mock_discovery
            
            result = await discover_causal_structure(
                variables=variable_descriptions,
                method=DiscoveryMethod.PC_ALGORITHM,
                domain_context="test domain"
            )
            
            assert result is mock_result
            mock_discovery.discover.assert_called_once()


class TestCoreCapabilities:
    """Test core capabilities mentioned in documentation."""
    
    @patch('causallm.core.create_llm_client')
    def test_hybrid_intelligence_capability(self, mock_create_client):
        """Test hybrid LLM + statistical intelligence capability."""
        mock_client = Mock()
        mock_create_client.return_value = mock_client
        
        causallm = CausalLLM(method="hybrid")
        
        # Should have both LLM and statistical components
        assert causallm.llm_client is not None
        assert causallm.discovery_engine is not None
        
        # Should support different discovery methods
        assert hasattr(causallm.discovery_engine, 'method')
    
    def test_statistical_validation_capability(self, sample_data):
        """Test rigorous statistical validation capability."""
        # PC Algorithm for structure learning
        ci_test = ConditionalIndependenceTest(method="partial_correlation", alpha=0.05)
        pc = PCAlgorithm(ci_test=ci_test, max_conditioning_size=2)
        
        # Should be able to discover structure
        skeleton = pc.discover_skeleton(sample_data)
        dag = pc.orient_edges(skeleton, sample_data)
        
        assert skeleton is not None
        assert dag is not None
        
        # Should support bootstrap validation
        stable_graph, scores = bootstrap_stability_test(sample_data, pc, n_bootstrap=10)
        
        assert stable_graph is not None
        assert isinstance(scores, dict)
    
    @patch('causallm.core.create_llm_client')
    def test_multiple_llm_providers_capability(self, mock_create_client):
        """Test support for multiple LLM providers."""
        from causallm.core.llm_client import get_llm_client
        
        # Should support different providers
        providers = ["openai", "llama", "grok", "mcp"]
        
        for provider in providers:
            with patch(f'causallm.core.llm_client.{provider.title()}Client') as mock_client_class:
                if provider == "mcp":
                    mock_client_class = patch('causallm.core.llm_client.MCPClient')
                    
                mock_client = Mock()
                mock_client_class.return_value = mock_client
                
                try:
                    client = get_llm_client(provider, "test-model")
                    assert client is not None
                except (ValueError, ImportError):
                    # Some providers might not be available in test environment
                    pass
    
    @pytest.mark.asyncio
    @patch('causallm.core.create_llm_client') 
    async def test_async_support_capability(self, mock_create_client):
        """Test async processing capability."""
        mock_client = Mock()
        mock_create_client.return_value = mock_client
        
        causallm = CausalLLM()
        
        # All main methods should be async
        assert hasattr(causallm, 'discover_causal_relationships')
        assert hasattr(causallm, 'estimate_causal_effect')
        assert hasattr(causallm, 'generate_counterfactuals')
        
        # Should be able to call async methods
        mock_result = Mock()
        causallm.discovery_engine.discover_relationships = AsyncMock(return_value=mock_result)
        
        result = await causallm.discover_causal_relationships(data=None, variables=["A", "B"])
        assert result is mock_result


class TestArchitectureComponents:
    """Test architecture components mentioned in documentation."""
    
    def test_core_components_available(self):
        """Test that all core components mentioned in README are available."""
        # Test imports of core components
        from causallm.core import causal_discovery
        from causallm.core import statistical_methods
        from causallm.core import dag_parser
        from causallm.core import do_operator
        from causallm.core import llm_client
        
        # Core classes should be available
        assert hasattr(causal_discovery, 'AdvancedCausalDiscovery')
        assert hasattr(statistical_methods, 'PCAlgorithm')
        assert hasattr(statistical_methods, 'ConditionalIndependenceTest')
        assert hasattr(dag_parser, 'DAGParser')
        assert hasattr(do_operator, 'DoOperatorSimulator')
        assert hasattr(llm_client, 'OpenAIClient')
        assert hasattr(llm_client, 'LLaMAClient')
    
    def test_plugin_system_available(self):
        """Test that plugin system components are available."""
        # Test plugin imports
        try:
            from causallm.plugins import slm_support
            assert hasattr(slm_support, 'create_slm_optimized_client')
        except ImportError:
            pytest.skip("Plugin system not fully implemented")
    
    def test_mcp_integration_available(self):
        """Test that MCP integration is available."""
        try:
            from causallm.mcp import client, server, tools
            assert hasattr(client, 'MCPLLMClient')
            assert hasattr(server, 'CausalLLMServer') 
            assert hasattr(tools, 'CausalTools')
        except ImportError:
            pytest.skip("MCP integration not fully implemented")


class TestUseCaseExamples:
    """Test use case examples from documentation."""
    
    @pytest.mark.asyncio
    @patch('causallm.core.create_llm_client')
    async def test_healthcare_use_case(self, mock_create_client, sample_data):
        """Test healthcare use case example."""
        mock_client = Mock()
        mock_create_client.return_value = mock_client
        
        causallm = CausalLLM()
        
        # Mock confounder detection result
        mock_confounders = Mock()
        mock_confounders.detected_confounders = ["age", "baseline_health"]
        
        with patch.object(causallm, 'detect_confounders', return_value=mock_confounders):
            # Clinical trial confounder detection
            confounders = await causallm.detect_confounders(
                data=sample_data,
                treatment="drug_dosage",
                outcome="recovery_time",
                domain="healthcare"
            )
            
            assert confounders is not None
            assert hasattr(confounders, 'detected_confounders')
    
    @pytest.mark.asyncio
    @patch('causallm.core.create_llm_client')
    async def test_business_use_case(self, mock_create_client, sample_data):
        """Test business & marketing use case example."""
        mock_client = Mock()
        mock_create_client.return_value = mock_client
        
        causallm = CausalLLM()
        
        # Mock attribution analysis result
        mock_attribution = Mock()
        mock_attribution.effect_estimate = 0.15
        mock_attribution.confidence_interval = [0.08, 0.22]
        
        causallm.do_operator.estimate_effect = AsyncMock(return_value=mock_attribution)
        
        # Marketing attribution analysis
        attribution = await causallm.estimate_causal_effect(
            data=sample_data,
            treatment="ad_spend",
            outcome="conversions",
            confounders=["seasonality", "brand_awareness"]
        )
        
        assert attribution is not None
        assert hasattr(attribution, 'effect_estimate')
    
    @pytest.mark.asyncio
    @patch('causallm.core.create_llm_client')
    async def test_policy_use_case(self, mock_create_client, sample_data):
        """Test economics & policy use case example."""
        mock_client = Mock()
        mock_create_client.return_value = mock_client
        
        causallm = CausalLLM()
        
        # Mock policy intervention analysis
        mock_policy_effect = Mock()
        mock_policy_effect.effect_estimate = -0.03
        mock_policy_effect.time_series_effects = [0.01, -0.02, -0.03]
        
        with patch.object(causallm, 'analyze_intervention', return_value=mock_policy_effect):
            # Policy intervention analysis
            policy_effect = await causallm.analyze_intervention(
                data=sample_data,
                intervention="minimum_wage_increase",
                outcome="employment_rate",
                time_variable="quarter"
            )
            
            assert policy_effect is not None
            assert hasattr(policy_effect, 'effect_estimate')


class TestPerformanceCharacteristics:
    """Test performance characteristics mentioned in documentation."""
    
    @pytest.mark.slow
    def test_large_dataset_handling(self, large_sample_data):
        """Test handling of datasets up to 1M+ rows as claimed."""
        # Test with large sample data
        assert len(large_sample_data) == 5000  # Our test data
        
        # Should be able to create PC algorithm and run basic operations
        ci_test = ConditionalIndependenceTest(method="partial_correlation")
        pc = PCAlgorithm(ci_test=ci_test, max_conditioning_size=2)
        
        import time
        start_time = time.time()
        
        skeleton = pc.discover_skeleton(large_sample_data)
        
        duration = time.time() - start_time
        
        # Should complete in reasonable time
        assert duration < 30  # Should finish within 30 seconds
        assert skeleton is not None
    
    @pytest.mark.asyncio
    @patch('causallm.core.create_llm_client')
    async def test_async_scalability(self, mock_create_client):
        """Test async processing scalability."""
        import asyncio
        
        mock_client = Mock()
        mock_create_client.return_value = mock_client
        
        causallm = CausalLLM()
        
        # Mock async operations
        mock_result = Mock()
        causallm.discovery_engine.discover_relationships = AsyncMock(return_value=mock_result)
        
        # Should be able to run multiple operations concurrently
        tasks = []
        for i in range(5):
            task = causallm.discover_causal_relationships(
                data=None,
                variables=[f"var_{i}", f"var_{i+1}"]
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        
        assert len(results) == 5
        assert all(r is mock_result for r in results)


class TestEnterpriseFeatures:
    """Test enterprise features detection."""
    
    @patch('causallm.core.create_llm_client')
    def test_enterprise_info_availability(self, mock_create_client):
        """Test that enterprise information is available."""
        mock_client = Mock()
        mock_create_client.return_value = mock_client
        
        causallm = CausalLLM()
        enterprise_info = causallm.get_enterprise_info()
        
        assert isinstance(enterprise_info, dict)
        assert 'licensed' in enterprise_info
        assert 'features' in enterprise_info
        assert 'info' in enterprise_info
        
        # Should indicate available benefits
        assert 'benefits' in enterprise_info
        assert isinstance(enterprise_info['benefits'], list)
        assert len(enterprise_info['benefits']) > 0
    
    @patch('causallm.core.create_llm_client')
    def test_open_source_limitations(self, mock_create_client):
        """Test that open source version properly indicates limitations."""
        mock_client = Mock()
        mock_create_client.return_value = mock_client
        
        causallm = CausalLLM()
        enterprise_info = causallm.get_enterprise_info()
        
        # Open source version should not be licensed
        assert enterprise_info['licensed'] is False
        
        # Should provide information about enterprise features
        assert "enterprise" in enterprise_info['info'].lower()
        
        # Should list enterprise benefits
        benefits = enterprise_info['benefits']
        expected_benefits = [
            "security", "scaling", "monitoring", "model", "compliance", "cloud", "support"
        ]
        
        benefit_text = " ".join(benefits).lower()
        for expected in expected_benefits:
            assert expected in benefit_text


class TestVersionAndCompatibility:
    """Test version information and compatibility."""
    
    def test_version_information_available(self):
        """Test that version information is available."""
        import causallm
        
        assert hasattr(causallm, '__version__')
        assert isinstance(causallm.__version__, str)
        assert len(causallm.__version__) > 0
        
        # Version should follow semantic versioning pattern
        version_parts = causallm.__version__.split('.')
        assert len(version_parts) >= 2  # At least major.minor
    
    def test_license_information(self):
        """Test that license information is available."""
        import causallm
        
        assert hasattr(causallm, '__license__')
        assert causallm.__license__ == "MIT"
    
    def test_author_information(self):
        """Test that author information is available."""
        import causallm
        
        assert hasattr(causallm, '__author__')
        assert isinstance(causallm.__author__, str)
        assert len(causallm.__author__) > 0
    
    def test_main_exports_available(self):
        """Test that main exports are available as documented."""
        import causallm
        
        # Main class should be available
        assert hasattr(causallm, 'CausalLLM')
        
        # Core components should be available
        expected_exports = [
            'CausalDiscoveryEngine',
            'DAGParser',
            'DoOperator', 
            'CounterfactualEngine',
            'CausalGraph',
            'InterventionResult'
        ]
        
        for export in expected_exports:
            try:
                assert hasattr(causallm, export)
            except AssertionError:
                pytest.skip(f"Export {export} not yet implemented")


@pytest.mark.integration
class TestDocumentationAccuracy:
    """Verify that the documentation accurately reflects the implementation."""
    
    def test_readme_quick_start_accuracy(self):
        """Verify README quick start example works as documented."""
        # The quick start should work without errors
        from causallm import CausalLLM
        
        # Should be able to initialize
        causallm = CausalLLM()
        assert causallm is not None
        
        # Should have the expected interface
        assert hasattr(causallm, 'discover_causal_relationships')
        assert hasattr(causallm, 'estimate_causal_effect')
        assert hasattr(causallm, 'generate_counterfactuals')
    
    def test_core_features_accuracy(self):
        """Verify that core features are implemented as documented."""
        # Hybrid Intelligence
        assert hasattr(CausalLLM, '__init__')
        
        # Statistical Methods
        from causallm.core.statistical_methods import PCAlgorithm, ConditionalIndependenceTest
        assert PCAlgorithm is not None
        assert ConditionalIndependenceTest is not None
        
        # Multiple LLM Providers
        from causallm.core.llm_client import get_llm_client
        assert get_llm_client is not None
    
    def test_installation_requirements_accuracy(self):
        """Verify that installation requirements match setup.py."""
        from causallm import __version__
        
        # Should have version that matches setup.py
        # Version should be imported from centralized _version.py
        from causallm._version import __version__ as expected_version
        assert __version__ == expected_version
        
        # Core dependencies should be importable
        required_packages = [
            'numpy', 'pandas', 'networkx', 'scipy'
        ]
        
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                pytest.fail(f"Required package {package} not available")


@pytest.mark.integration
class TestFullWorkflow:
    """Test complete workflows as a user would experience them."""
    
    @pytest.mark.asyncio
    @patch('causallm.core.create_llm_client')
    async def test_complete_causal_analysis_workflow(self, mock_create_client, sample_data, variable_descriptions):
        """Test a complete causal analysis workflow."""
        mock_client = Mock()
        mock_create_client.return_value = mock_client
        
        # Step 1: Initialize CausalLLM
        causallm = CausalLLM()
        
        # Step 2: Discover causal structure
        mock_structure = Mock()
        mock_structure.discovered_edges = []
        causallm.discovery_engine.discover_relationships = AsyncMock(return_value=mock_structure)
        
        structure = await causallm.discover_causal_relationships(
            data=sample_data,
            variables=list(variable_descriptions.keys())
        )
        
        # Step 3: Estimate causal effects
        mock_effect = Mock() 
        mock_effect.estimate = 0.42
        mock_effect.std_error = 0.05
        causallm.do_operator.estimate_effect = AsyncMock(return_value=mock_effect)
        
        effect = await causallm.estimate_causal_effect(
            data=sample_data,
            treatment="X1",
            outcome="X3"
        )
        
        # Step 4: Generate counterfactuals
        mock_counterfactual = Mock()
        mock_counterfactual.counterfactual_scenarios = ["Scenario A", "Scenario B"]
        causallm.counterfactual_engine.generate_counterfactuals = AsyncMock(return_value=mock_counterfactual)
        
        counterfactual = await causallm.generate_counterfactuals(
            data=sample_data,
            intervention={"X1": "high_treatment"}
        )
        
        # Verify complete workflow
        assert structure is not None
        assert effect is not None
        assert counterfactual is not None
        assert hasattr(effect, 'estimate')
        assert hasattr(effect, 'std_error')
    
    def test_statistical_only_workflow(self, sample_data):
        """Test workflow using only statistical methods (no LLM)."""
        # Step 1: Set up statistical methods
        ci_test = ConditionalIndependenceTest(method="partial_correlation")
        pc = PCAlgorithm(ci_test=ci_test, max_conditioning_size=2)
        
        # Step 2: Discover structure
        skeleton = pc.discover_skeleton(sample_data)
        dag = pc.orient_edges(skeleton, sample_data)
        
        # Step 3: Validate with bootstrap
        stable_graph, stability = bootstrap_stability_test(
            sample_data, pc, n_bootstrap=10
        )
        
        # Verify statistical workflow
        assert skeleton is not None
        assert dag is not None
        assert stable_graph is not None
        assert isinstance(stability, dict)